<div align="center">
  <img src="../09-databricks-ai-project/assets/jornada.png" alt="Jornada de Dados" width="200"/>

# **Projeto: Data Quality Engineering**

### [Jornada de Dados](https://suajornadadedados.com.br/)

**Projeto prÃ¡tico sobre pipelines de qualidade de dados com validaÃ§Ã£o de schemas e contratos de dados**

[![Projeto](https://img.shields.io/badge/Projeto-Data%20Quality-blue?style=for-the-badge)](https://suajornadadedados.com.br/)
[![Python](https://img.shields.io/badge/Python-3.11+-green?style=for-the-badge\&logo=python)](https://python.org)
[![Pandera](https://img.shields.io/badge/Pandera-Schema%20Validation-purple?style=for-the-badge)](https://pandera.readthedocs.io/)
[![DuckDB](https://img.shields.io/badge/DuckDB-Analytics-yellow?style=for-the-badge)](https://duckdb.org/)
[![PostgreSQL](https://img.shields.io/badge/PostgreSQL-Database-blue?style=for-the-badge\&logo=postgresql)](https://www.postgresql.org/)

</div>

-----

## ğŸ“‹ Sobre

Este projeto demonstra como implementar **pipelines de qualidade de dados** utilizando validaÃ§Ã£o de schemas, contratos de dados e tratamento de erros. O projeto implementa uma pipeline ETL que extrai dados de PostgreSQL, valida e transforma usando Pandera, e carrega em DuckDB para anÃ¡lise.

**Objetivo Educacional**: Aprender a garantir qualidade de dados em pipelines ETL atravÃ©s de validaÃ§Ã£o de schemas, contratos de dados e tratamento de erros robusto.

## ğŸ“Š Fluxo do Projeto

```mermaid
graph LR
    A[PostgreSQL<br/>Dados Brutos] -->|Extract| B[ValidaÃ§Ã£o<br/>Pandera Schema]
    B -->|Transform| C[CÃ¡lculo KPIs<br/>NormalizaÃ§Ã£o]
    C -->|ValidaÃ§Ã£o| D[ValidaÃ§Ã£o<br/>Schema KPI]
    D -->|Load| E[DuckDB<br/>Dados Validados]
    
    F[InferÃªncia<br/>Schema AutomÃ¡tico] --> B
    
    style A fill:#e1f5ff
    style B fill:#fff3e0
    style C fill:#f3e5f5
    style D fill:#fff3e0
    style E fill:#c8e6c9
    style F fill:#e8f5e9
```

## ğŸ¯ Objetivos de Aprendizado

- **ValidaÃ§Ã£o de Schemas**: Usar Pandera para validar estruturas e tipos de dados
- **Contratos de Dados**: Definir contratos que garantem qualidade dos dados
- **Data Profiling**: Inferir schemas automaticamente a partir de dados existentes
- **ETL com Qualidade**: Integrar validaÃ§Ã£o em cada etapa da pipeline ETL
- **DuckDB**: Usar DuckDB como banco de dados analÃ­tico para armazenar dados validados
- **Tratamento de Erros**: Implementar tratamento robusto de erros de validaÃ§Ã£o

## ğŸ“ Estrutura do Projeto

```
04-data-quality-engineering/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ etl.py                    # Pipeline ETL principal com validaÃ§Ã£o
â”‚   â”œâ”€â”€ etl_infer_schema.py      # InferÃªncia automÃ¡tica de schemas
â”‚   â”œâ”€â”€ schema.py                 # DefiniÃ§Ã£o de schemas com Pandera
â”‚   â”œâ”€â”€ schema_email.py           # ValidaÃ§Ã£o especÃ­fica de emails
â”‚   â””â”€â”€ ler_duckdb.py             # Leitura de dados do DuckDB
â”œâ”€â”€ sql/                          # Scripts SQL para criaÃ§Ã£o de tabelas
â”œâ”€â”€ tests/                        # Testes automatizados
â”œâ”€â”€ docs/                         # DocumentaÃ§Ã£o adicional
â”œâ”€â”€ inferred_schema.json          # Schema inferido automaticamente
â”œâ”€â”€ my_duckdb.db                  # Banco DuckDB (gerado apÃ³s execuÃ§Ã£o)
â”œâ”€â”€ schema_crm.py                 # Schema do CRM
â””â”€â”€ README.md                      # Este arquivo
```

## ğŸ› ï¸ Tecnologias e Ferramentas

- **Python 3.x**: Linguagem de programaÃ§Ã£o
- **Pandera**: ValidaÃ§Ã£o de schemas e contratos de dados
- **Pandas**: ManipulaÃ§Ã£o e transformaÃ§Ã£o de dados
- **DuckDB**: Banco de dados analÃ­tico em memÃ³ria
- **PostgreSQL**: Banco de dados fonte (opcional, pode usar dados locais)
- **SQLAlchemy**: ConexÃ£o com bancos de dados SQL
- **python-dotenv**: Gerenciamento de variÃ¡veis de ambiente

## ğŸ“¦ PrÃ©-requisitos

- Python 3.11+ instalado
- Poetry instalado (ou pip)
- PostgreSQL instalado e configurado (opcional)
- Conhecimento bÃ¡sico de Python e SQL

## ğŸš€ Como Usar

### InstalaÃ§Ã£o

1. **Clone o repositÃ³rio**:
   ```bash
   git clone https://github.com/lvgalvao/data-engineering-roadmap.git
   cd data-engineering-roadmap/01-projetos/04-data-quality-engineering
   ```

2. **Instale as dependÃªncias**:
   ```bash
   poetry install
   # ou
   pip install -r requirements.txt
   ```

3. **Configure variÃ¡veis de ambiente** (se usar PostgreSQL):
   Crie um arquivo `.env` com:
   ```env
   POSTGRES_HOST=localhost
   POSTGRES_USER=user
   POSTGRES_PASSWORD=password
   POSTGRES_DB=database
   POSTGRES_PORT=5432
   ```

### ExecuÃ§Ã£o

1. **Execute a pipeline ETL com validaÃ§Ã£o**:
   ```bash
   python app/etl.py
   ```

2. **Inferir schema automaticamente**:
   ```bash
   python app/etl_infer_schema.py
   ```

3. **Ler dados do DuckDB**:
   ```bash
   python app/ler_duckdb.py
   ```

## ğŸ“š ConteÃºdo Real

### Pipeline ETL com ValidaÃ§Ã£o

O projeto implementa uma pipeline ETL completa com validaÃ§Ã£o em cada etapa:

1. **Extract (`extrair_do_sql`)**:
   - Extrai dados de PostgreSQL usando SQLAlchemy
   - Valida dados de saÃ­da usando `@pa.check_output(ProdutoSchema)`
   - Retorna DataFrame validado

2. **Transform (`transformar`)**:
   - Valida dados de entrada usando `@pa.check_input(ProdutoSchema)`
   - Calcula KPIs: `valor_total_estoque = quantidade * preco`
   - Normaliza categorias para minÃºsculas
   - Determina disponibilidade baseado em quantidade
   - Valida dados de saÃ­da usando `@pa.check_output(ProductSchemaKPI)`

3. **Load (`load_to_duckdb`)**:
   - Valida dados de entrada usando `@pa.check_input(ProductSchemaKPI)`
   - Carrega dados validados no DuckDB
   - Cria ou substitui tabela no DuckDB

### Schemas e ValidaÃ§Ã£o

O projeto define schemas usando Pandera:

- **`ProdutoSchema`**: Schema para dados brutos extraÃ­dos do PostgreSQL
  - Valida tipos, valores obrigatÃ³rios, formatos
  - Inclui validaÃ§Ã£o de emails usando regex

- **`ProductSchemaKPI`**: Schema para dados transformados
  - Valida campos calculados
  - Garante consistÃªncia dos KPIs

### InferÃªncia de Schema

O mÃ³dulo `etl_infer_schema.py` demonstra como:
- Inferir schema automaticamente a partir de dados existentes
- Gerar arquivo JSON com schema inferido
- Usar schema inferido para validaÃ§Ã£o

### ValidaÃ§Ã£o de Emails

O mÃ³dulo `schema_email.py` implementa validaÃ§Ã£o especÃ­fica de emails usando:
- ExpressÃµes regulares (regex)
- ValidaÃ§Ã£o de formato
- Tratamento de casos especiais

## ğŸ”— ConexÃµes com a FormaÃ§Ã£o

- **PrÃ©-requisitos**: 
  - Projeto 01 (Data Project Foundations) para entender estruturaÃ§Ã£o de projetos
  - Projeto 02 (Python Big Data Processing) para entender DuckDB
  - Conhecimento bÃ¡sico de Python e SQL
- **PrÃ³ximos passos**: 
  - Projeto 08 (Databricks Data Modeling) para aplicar qualidade em ambientes cloud
  - MÃ³dulo de dbt em `04-sql-analytics-dbt-core/` para validaÃ§Ã£o com dbt

## ğŸ“– Recursos Adicionais

- [DocumentaÃ§Ã£o do Pandera](https://pandera.readthedocs.io/)
- [DocumentaÃ§Ã£o do DuckDB](https://duckdb.org/docs/)
- [Best Practices de Data Quality](https://www.getdbt.com/blog/data-quality)

## ğŸ‘¤ Autor

**Luciano Filho** - [lvgalvaofilho@gmail.com](mailto:lvgalvaofilho@gmail.com)

---

**Parte da FormaÃ§Ã£o Profissional em Engenharia de Dados - [Jornada de Dados](https://suajornadadedados.com.br/)**
