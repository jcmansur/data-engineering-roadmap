# Web Scraping NoSQL Pipelines

## ğŸ“‹ Sobre

Este projeto demonstra como construir **pipelines de web scraping** utilizando Python com armazenamento em bancos de dados NoSQL (Redis e MongoDB). O projeto implementa crawlers genÃ©ricos que podem ser adaptados para diferentes sites, utilizando tanto requisiÃ§Ãµes HTTP simples quanto navegadores automatizados (Selenium).

**Objetivo Educacional**: Aprender tÃ©cnicas de web scraping, trabalhar com bancos NoSQL, implementar crawlers reutilizÃ¡veis e gerenciar cache com Redis.

## ğŸ“Š Fluxo do Projeto

```mermaid
graph LR
    A[Websites<br/>ML, Amazon] -->|Scraping| B{Crawler}
    
    B -->|HTTP Requests| C1[GenericRequestCrawler<br/>Mais RÃ¡pido]
    B -->|Selenium Browser| C2[GenericBrowserCrawler<br/>Mais Completo]
    
    C1 --> D[Redis<br/>Cache]
    C2 --> D
    
    D --> E[MongoDB<br/>Armazenamento]
    
    style A fill:#e1f5ff
    style B fill:#fff3e0
    style D fill:#ffebee
    style E fill:#c8e6c9
```

## ğŸ¯ Objetivos de Aprendizado

- **Web Scraping**: Extrair dados de websites usando Requests e Selenium
- **Crawlers GenÃ©ricos**: Criar crawlers reutilizÃ¡veis para mÃºltiplos sites
- **Redis**: Usar Redis como cache e message broker
- **MongoDB**: Armazenar dados nÃ£o-estruturados em banco NoSQL
- **Arquitetura Modular**: Organizar cÃ³digo em mÃ³dulos reutilizÃ¡veis
- **AutomaÃ§Ã£o de Navegadores**: Controlar navegadores com Selenium

## ğŸ“ Estrutura do Projeto

```
06-web-scraping-nosql-pipelines/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ browser/              # Crawlers baseados em navegador (Selenium)
â”‚   â”‚   â”œâ”€â”€ generic_crawler.py
â”‚   â”‚   â”œâ”€â”€ b_ml_simple.py    # Exemplo: Mercado Livre
â”‚   â”‚   â””â”€â”€ crawlers/         # ImplementaÃ§Ãµes especÃ­ficas
â”‚   â”œâ”€â”€ request/              # Crawlers baseados em requisiÃ§Ãµes HTTP
â”‚   â”‚   â”œâ”€â”€ generic_crawler.py
â”‚   â”‚   â”œâ”€â”€ r_ml_simple.py    # Exemplo: Mercado Livre
â”‚   â”‚   â””â”€â”€ crawlers/         # ImplementaÃ§Ãµes especÃ­ficas
â”‚   â”œâ”€â”€ mitm/                 # Proxy e interceptaÃ§Ã£o de trÃ¡fego
â”‚   â”œâ”€â”€ tools/
â”‚   â”‚   â”œâ”€â”€ mongodb.py        # UtilitÃ¡rios para MongoDB
â”‚   â”‚   â””â”€â”€ redis.py          # UtilitÃ¡rios para Redis
â”‚   â””â”€â”€ start.py              # Ponto de entrada
â”œâ”€â”€ docker/                   # ConfiguraÃ§Ãµes Docker
â”œâ”€â”€ pyproject.toml            # DependÃªncias
â””â”€â”€ README.md                 # Este arquivo
```

## ğŸ› ï¸ Tecnologias e Ferramentas

- **Python 3.x**: Linguagem de programaÃ§Ã£o
- **Requests**: Biblioteca para requisiÃ§Ãµes HTTP
- **BeautifulSoup**: Parsing de HTML
- **Selenium**: AutomaÃ§Ã£o de navegadores
- **Scrapy**: Framework para web scraping (opcional)
- **Redis**: Cache e message broker em memÃ³ria
- **MongoDB**: Banco de dados NoSQL orientado a documentos
- **Docker**: ContainerizaÃ§Ã£o (opcional)

## ğŸ“¦ PrÃ©-requisitos

- Python 3.11+ instalado
- Poetry instalado (ou pip)
- Redis instalado e rodando (ou via Docker)
- MongoDB instalado e rodando (ou via Docker)
- Chrome/Chromium instalado (para crawlers com Selenium)
- Conhecimento bÃ¡sico de Python e HTML

## ğŸš€ Como Usar

### InstalaÃ§Ã£o

1. **Clone o repositÃ³rio**:
   ```bash
   git clone https://github.com/lvgalvao/data-engineering-roadmap.git
   cd data-engineering-roadmap/01-projetos/06-web-scraping-nosql-pipelines
   ```

2. **Instale as dependÃªncias**:
   ```bash
   poetry install
   # ou
   pip install -r requirements.txt
   ```

3. **Inicie Redis e MongoDB** (se usar Docker):
   ```bash
   docker-compose -f docker/docker-compose.yml up -d
   ```

### ExecuÃ§Ã£o

1. **Execute o crawler de exemplo**:
   ```bash
   python src/start.py
   ```

2. **Ou use os crawlers individuais**:
   ```bash
   # Crawler com navegador (Selenium)
   python src/browser/b_ml_simple.py
   
   # Crawler com requisiÃ§Ãµes HTTP
   python src/request/r_ml_simple.py
   ```

## ğŸ“š ConteÃºdo Real

### Crawlers GenÃ©ricos

O projeto implementa dois tipos de crawlers genÃ©ricos:

1. **`GenericBrowserCrawler`** (`src/browser/generic_crawler.py`):
   - Usa Selenium para controlar navegadores
   - Ãštil para sites com JavaScript pesado
   - Mais lento, mas mais completo

2. **`GenericRequestCrawler`** (`src/request/generic_crawler.py`):
   - Usa Requests para requisiÃ§Ãµes HTTP diretas
   - Mais rÃ¡pido e eficiente
   - Ideal para sites estÃ¡ticos

### Exemplos Implementados

- **Mercado Livre**: Crawlers para buscar produtos no ML
- **Amazon**: Crawlers para buscar produtos na Amazon

### IntegraÃ§Ã£o com NoSQL

- **Redis** (`src/tools/redis.py`):
  - Cache de requisiÃ§Ãµes
  - Fila de mensagens
  - Armazenamento temporÃ¡rio

- **MongoDB** (`src/tools/mongodb.py`):
  - Armazenamento persistente de dados extraÃ­dos
  - Estrutura flexÃ­vel para dados nÃ£o-estruturados

### Proxy e InterceptaÃ§Ã£o

O mÃ³dulo `mitm/` implementa:
- Proxy para interceptar requisiÃ§Ãµes
- Controle de trÃ¡fego de rede
- Testes de conectividade

## ğŸ”— ConexÃµes com a FormaÃ§Ã£o

- **PrÃ©-requisitos**: 
  - Projeto 01 (Data Project Foundations) para entender estruturaÃ§Ã£o
  - Conhecimento bÃ¡sico de Python e HTML
- **PrÃ³ximos passos**: 
  - Projeto 07 (PDF Data Extraction) para extraÃ§Ã£o de dados nÃ£o-estruturados
  - MÃ³dulo de APIs em `05-engenharia-de-dados-e-ia/06-restAPI-fastAPI-deploy/`

## ğŸ“– Recursos Adicionais

- [DocumentaÃ§Ã£o do Requests](https://requests.readthedocs.io/)
- [DocumentaÃ§Ã£o do Selenium](https://www.selenium.dev/documentation/)
- [DocumentaÃ§Ã£o do BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)
- [DocumentaÃ§Ã£o do Redis](https://redis.io/docs/)
- [DocumentaÃ§Ã£o do MongoDB](https://www.mongodb.com/docs/)

## ğŸ‘¤ Autor

**Luciano Filho** - [lvgalvaofilho@gmail.com](mailto:lvgalvaofilho@gmail.com)

---

**Parte da FormaÃ§Ã£o Profissional em Engenharia de Dados - [Jornada de Dados](https://suajornadadedados.com.br/)**
